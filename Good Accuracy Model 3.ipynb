{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSPCom-KmApV"
   },
   "source": [
    "# Complex Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iAve6DCL4JH4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0-dev20201211\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['training_d']>\n",
      "<KeysViewHDF5 ['train_labels_d']>\n"
     ]
    }
   ],
   "source": [
    "# X training\n",
    "f = h5py.File(\"./X_training_serial_balanced.h5\",\"r\")\n",
    "print(f.keys())\n",
    "train_images = f['training_d'][:]\n",
    "f.close()\n",
    "train_images[np.isnan(train_images)] = 0.\n",
    "\n",
    "# Y training\n",
    "f = h5py.File(\"./Y_training_serial_balanced.h5\",\"r\")\n",
    "print(f.keys())\n",
    "train_labels = f['train_labels_d'][:].astype(int)\n",
    "f.close()\n",
    "\n",
    "# X validation\n",
    "f = h5py.File(\"./X_validation.h5\",\"r\")\n",
    "val_images = f['validation'][:]\n",
    "f.close()\n",
    "val_images[np.isnan(val_images)] = 0.\n",
    "\n",
    "# Y validation\n",
    "f = h5py.File(\"./Y_validation.h5\",\"r\")\n",
    "val_labels = f['val_labels'][:].astype(int)\n",
    "f.close()\n",
    "\n",
    "# X testing\n",
    "f = h5py.File(\"./X_testing.h5\",\"r\")\n",
    "test_images = f['testing'][:]\n",
    "f.close()\n",
    "test_images[np.isnan(test_images)] = 0.\n",
    "\n",
    "# Y testing\n",
    "f = h5py.File(\"./Y_testing.h5\",\"r\")\n",
    "test_labels = f['test_labels'][:].astype(int)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112700, 32, 32, 3)\n",
      "(112700, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_images[0][0][0][0]))\n",
    "\n",
    "print(type(train_labels[0][0]))\n",
    "\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "val_images = val_images.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_images[0][0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22540 22540 22540 22540 22540]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# Number of occurences of each class that occur before\n",
    "a = np.unique(train_labels,return_counts =1, return_index=1, return_inverse = 1)\n",
    "print(a[3])\n",
    "\n",
    "print(train_labels[3:13])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRFxccghyMVo"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "L9YmGQBQPrdn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 95,813\n",
      "Trainable params: 95,813\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Simple CNN Network\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MdDzI75PUXrG",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alkinoos\\anaconda3\\envs\\NEW\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:5008: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112700/112700 [==============================] - 1012s 9ms/step - loss: 1.5902 - accuracy: 0.2463 - val_loss: 1.6369 - val_accuracy: 0.3222\n",
      "Epoch 2/20\n",
      "112700/112700 [==============================] - 847s 8ms/step - loss: 1.5570 - accuracy: 0.2813 - val_loss: 1.6543 - val_accuracy: 0.2850\n",
      "Epoch 3/20\n",
      "112700/112700 [==============================] - 961s 9ms/step - loss: 1.5435 - accuracy: 0.2905 - val_loss: 1.9214 - val_accuracy: 0.2222\n",
      "Epoch 4/20\n",
      "112700/112700 [==============================] - 1038s 9ms/step - loss: 1.5318 - accuracy: 0.3012 - val_loss: 1.8699 - val_accuracy: 0.1900\n",
      "Epoch 5/20\n",
      "112700/112700 [==============================] - 927s 8ms/step - loss: 1.5220 - accuracy: 0.3094 - val_loss: 2.0521 - val_accuracy: 0.1978\n",
      "Epoch 6/20\n",
      "112700/112700 [==============================] - 994s 9ms/step - loss: 1.5151 - accuracy: 0.3140 - val_loss: 1.9106 - val_accuracy: 0.2228\n",
      "Epoch 7/20\n",
      "112700/112700 [==============================] - 937s 8ms/step - loss: 1.5087 - accuracy: 0.3168 - val_loss: 1.6833 - val_accuracy: 0.3089\n",
      "Epoch 8/20\n",
      "112700/112700 [==============================] - 993s 9ms/step - loss: 1.5034 - accuracy: 0.3185 - val_loss: 1.8850 - val_accuracy: 0.2156\n",
      "Epoch 9/20\n",
      "112700/112700 [==============================] - 1023s 9ms/step - loss: 1.4986 - accuracy: 0.3224 - val_loss: 1.7945 - val_accuracy: 0.2522\n",
      "Epoch 10/20\n",
      "112700/112700 [==============================] - 1110s 10ms/step - loss: 1.4964 - accuracy: 0.3220 - val_loss: 1.8177 - val_accuracy: 0.2528\n",
      "Epoch 11/20\n",
      "112700/112700 [==============================] - 787s 7ms/step - loss: 1.4924 - accuracy: 0.3294 - val_loss: 2.0195 - val_accuracy: 0.1767\n",
      "Epoch 12/20\n",
      "112700/112700 [==============================] - 550s 5ms/step - loss: 1.4907 - accuracy: 0.3303 - val_loss: 1.7451 - val_accuracy: 0.2689\n",
      "Epoch 13/20\n",
      "112700/112700 [==============================] - 897s 8ms/step - loss: 1.4884 - accuracy: 0.3309 - val_loss: 2.2218 - val_accuracy: 0.1922\n",
      "Epoch 14/20\n",
      "112700/112700 [==============================] - 529s 5ms/step - loss: 1.4824 - accuracy: 0.3349 - val_loss: 1.8896 - val_accuracy: 0.2206\n",
      "Epoch 15/20\n",
      "112700/112700 [==============================] - 492s 4ms/step - loss: 1.4778 - accuracy: 0.3346 - val_loss: 2.0358 - val_accuracy: 0.2017\n",
      "Epoch 16/20\n",
      "112700/112700 [==============================] - 484s 4ms/step - loss: 1.4767 - accuracy: 0.3360 - val_loss: 1.8244 - val_accuracy: 0.2461\n",
      "Epoch 17/20\n",
      "112700/112700 [==============================] - 490s 4ms/step - loss: 1.4730 - accuracy: 0.3412 - val_loss: 1.9030 - val_accuracy: 0.2444\n",
      "Epoch 18/20\n",
      "112700/112700 [==============================] - 487s 4ms/step - loss: 1.4698 - accuracy: 0.3404 - val_loss: 1.8293 - val_accuracy: 0.2472\n",
      "Epoch 19/20\n",
      "112700/112700 [==============================] - 539s 5ms/step - loss: 1.4678 - accuracy: 0.3432 - val_loss: 1.9655 - val_accuracy: 0.2311\n",
      "Epoch 20/20\n",
      "112700/112700 [==============================] - 333s 3ms/step - loss: 1.4634 - accuracy: 0.3454 - val_loss: 1.9777 - val_accuracy: 0.1956\n",
      "\n",
      "\n",
      "[0 3 0 0 1 4 3 1 4 4 4 4 3 4 3 4 1 0 4 2 0 0 1 1 4 4 4 3 0 1 4 3 3 3 1 3 3\n",
      " 1 1 1]\n",
      "\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "57/57 - 1s - loss: 1.7893 - accuracy: 0.2550\n",
      "\n",
      "\n",
      "0.2549999952316284\n",
      "\n",
      "\n",
      "[0 1 2 3 4]\n",
      "[403 288  53 681 375]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(learning_rate=0.01),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=20, batch_size = 1,\n",
    "                    validation_data=(val_images, val_labels), shuffle=True)\n",
    "\n",
    "print(\"\\n\")\n",
    "# Predict on the test images\n",
    "predictions = model.predict(test_images)\n",
    "# Print our model's predictions.\n",
    "predicts = np.argmax(predictions, axis=1)\n",
    "print(predicts[0:40]) \n",
    "print(\"\\n\")\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[0:40].flatten())\n",
    "\n",
    "print(\"\\n\")\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "print(\"\\n\")\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(test_acc)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Number of occurences of each class that occur before\n",
    "b = np.unique(predicts,return_counts =1, return_index=1, return_inverse = 1)\n",
    "print(b[0])\n",
    "print(b[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"model_arch_4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Alkinoos\\AppData\\Local\\Temp\\tmpuggwupvs\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Alkinoos\\AppData\\Local\\Temp\\tmpuggwupvs\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "108016"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFLite Section\n",
    "\n",
    "\n",
    "tflite_model_name = 'MFCC_4'\n",
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "quantize = True\n",
    "if (quantize):\n",
    "    def representative_dataset():\n",
    "        for i in range(500):\n",
    "            yield([train_images[i].reshape(1,32,32,3)])\n",
    "    # Set the optimization flag.\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # Enforce full-int8 quantization\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    # Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(tflite_model_name + '.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Alkinoos\\AppData\\Local\\Temp\\tmpxgmoywq5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Alkinoos\\AppData\\Local\\Temp\\tmpxgmoywq5\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "108016"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GAPUINO Section\n",
    "\n",
    "\n",
    "tflite_model_name = 'quant_model_4'\n",
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "quantize = True\n",
    "if (quantize):\n",
    "    def representative_dataset():\n",
    "        for i in range(500):\n",
    "            yield([train_images[i].reshape(1,32,32,3)])\n",
    "    # Set the optimization flag.\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # Enforce full-int8 quantization\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    # Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(tflite_model_name + '.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: conv2d_6_input_int8\n",
      "shape: [ 1 32 32  3]\n",
      "type: <class 'numpy.int8'>\n",
      "\n",
      "== Output details ==\n",
      "name: Identity_int8\n",
      "shape: [1 5]\n",
      "type: <class 'numpy.int8'>\n"
     ]
    }
   ],
   "source": [
    "# This function here takes in the model and outputs a header file we will import into the TFLite example project folder. (/Core/Inc/)\n",
    "# Function: Convert some hex value into an array for C programming\n",
    "\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "\n",
    "    c_str = ''\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "    c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    # Add array length at top of file\n",
    "    c_str += '\\nunsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'unsigned char ' + var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data) :\n",
    "\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "            hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += '\\n '\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "    return c_str\n",
    "\n",
    "\n",
    "c_model_name = 'MFCC_4'\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open(c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model, c_model_name))\n",
    "    \n",
    "\n",
    "tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name + '.tflite')\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
